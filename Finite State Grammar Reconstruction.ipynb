{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite State Grammar Reconstruction\n",
    "*Sophie Libkind*\n",
    "\n",
    "This notebook explores the connection between the activations of an RNN trained to recognized a Finite State Grammar and the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import operator\n",
    "import random\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy = lambda ps: -sum([p*np.log2(p) for p in ps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite-State Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A finite state grammar is represented as a dictionary with \n",
    "#      key - the state\n",
    "#      value - a list of next states \n",
    "\n",
    "class FSG:\n",
    "\n",
    "    def __init__(self, fsg, begin):\n",
    "        '''\n",
    "        fsg - a finite state grammar of the kind expressed above\n",
    "        begin - the beginning state\n",
    "        '''\n",
    "        self.fsg = fsg\n",
    "        self.curr = begin\n",
    "        \n",
    "    def generate(self, n, reset_curr):\n",
    "        '''\n",
    "        Generates a word in the FSG of length n\n",
    "        Resets the current state of the FSG if the reset_curr flag is true\n",
    "        '''\n",
    "        curr = self.curr\n",
    "        output = []\n",
    "        while len(output) < n:\n",
    "            nexts = self.fsg[curr].keys()\n",
    "            ps = [self.fsg[curr][nx] for nx in nexts]\n",
    "            i = np.random.choice(range(len(nexts)), p = ps)\n",
    "            (nxt, out) = nexts[i]\n",
    "            output.append(out)\n",
    "            curr = nxt\n",
    "        \n",
    "        if reset_curr:\n",
    "            self.curr = curr\n",
    "        \n",
    "        return output \n",
    "    \n",
    "    def generate_with_probdists(self, n, reset_curr):\n",
    "        '''\n",
    "        Generates a word in the FSG of length n\n",
    "        Also returns the probability distribution over the next characters at each step\n",
    "        Resets the current state of the FSG if the reset_curr flag is true\n",
    "        '''\n",
    "        curr = self.curr\n",
    "        output = []\n",
    "        pdists = []\n",
    "            \n",
    "        while True:\n",
    "            nexts = self.fsg[curr].keys()\n",
    "            ps = [self.fsg[curr][nx] for nx in nexts]\n",
    "            \n",
    "            # accumulate probability distributions\n",
    "            pdist = collections.defaultdict(int)\n",
    "            for nx in nexts:\n",
    "                pdist[nx[1]] += self.fsg[curr][nx]\n",
    "            pdists.append(pdist)\n",
    "            \n",
    "            if len(output) > n:\n",
    "                break\n",
    "                \n",
    "            # choose next letter\n",
    "            i = np.random.choice(range(len(nexts)), p = ps)\n",
    "            (nxt, out) = nexts[i]\n",
    "            output.append(out)\n",
    "            curr = nxt\n",
    "\n",
    "        if reset_curr:\n",
    "            self.curr = curr\n",
    "        \n",
    "        return output, pdists\n",
    "    \n",
    "    def is_grammatical(self,word):\n",
    "        '''\n",
    "        Decides if the word is grammatical with respect to the FSG\n",
    "        '''\n",
    "        currs = [self.curr]\n",
    "        for letter in word:\n",
    "            if len(currs) == 0:\n",
    "                return False\n",
    "            nexts = []\n",
    "            for state in currs:\n",
    "                nexts += [trans[0] for trans in self.fsg[state].keys() if trans[1] == letter]\n",
    "            currs = nexts\n",
    "\n",
    "        if len(currs) > 0:\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "This simple RNN is based on Andrej Karpathy's Minimal character-level Vanilla RNN model at: https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, data, hidden_size):\n",
    "        self.training_chars = data        \n",
    "        self.hidden_size = hidden_size\n",
    "        # data I/O\n",
    "        chars = list(set(data))\n",
    "        data_size, self.vocab_size = len(data), len(chars)\n",
    "        print 'data has %d characters, %d unique.' % (data_size, self.vocab_size)\n",
    "        self.char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = 3 # size of hidden layer of neurons\n",
    "        seq_length = 25 # number of steps to unroll the RNN for\n",
    "        learning_rate = 1e-1\n",
    "        \n",
    "        # model parameters\n",
    "        self.Wxh = np.random.randn(self.hidden_size, self.vocab_size)*0.01 # input to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size)*0.01 # hidden to hidden\n",
    "        self.Why = np.random.randn(self.vocab_size, self.hidden_size)*0.01 # hidden to output\n",
    "        self.bh = np.zeros((self.hidden_size, 1)) # hidden bias\n",
    "        self.by = np.zeros((self.vocab_size, 1)) # output bias\n",
    "\n",
    "\n",
    "        n, p = 0, 0\n",
    "        mWxh, mWhh, mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        mbh, mby = np.zeros_like(self.bh), np.zeros_like(self.by) # memory variables for Adagrad\n",
    "        smooth_loss = -np.log(1.0/self.vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "        hprev = np.zeros((self.hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "        \n",
    "        while True:\n",
    "            seq_length = min(seq_length, len(data) - p - 1)\n",
    "                \n",
    "            inputs = [self.char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "            targets = [self.char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "            # forward seq_length characters through the net and fetch gradient\n",
    "            loss, dWxh, dWhh, dWhy, dbh, dby, hprev = self.lossFun(inputs, targets, hprev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            if n % 1000 == 0: \n",
    "                print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "\n",
    "            # perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by], \n",
    "                                        [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                        [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "                mem += dparam * dparam\n",
    "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "            p += seq_length # move data pointer\n",
    "            n += 1 # iteration counter \n",
    "            if (p == len(data) - 1):\n",
    "                break\n",
    "\n",
    "        clear_output()\n",
    "        print  'iter %d, loss: %f' % (n, smooth_loss)\n",
    "        \n",
    "    def lossFun(self, inputs, targets, hprev, reset_lasth = True):\n",
    "        \"\"\"\n",
    "        inputs,targets are both list of integers.\n",
    "        hprev is Hx1 array of initial hidden state\n",
    "        returns the loss, gradients on model parameters, and last hidden state\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        # forward pass\n",
    "        for t in xrange(len(inputs)):            \n",
    "            xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh) # hidden state\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "        \n",
    "        if reset_lasth:\n",
    "            self.last_h = hs[t]\n",
    "            \n",
    "        # backward pass: compute gradients going backwards\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(xrange(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext # backprop into h\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "    def sample(self, n):\n",
    "        \"\"\" \n",
    "        sample a sequence of integers from the model \n",
    "        h is memory state, seed_ix is seed letter for first time step\n",
    "        \"\"\"\n",
    "        h = self.last_h\n",
    "        seed_ix = self.char_to_ix[self.training_chars[-1]]\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        \n",
    "        for t in xrange(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            p = np.square(p) / np.sum(np.square(p)) # improves chances of grabbing a more likely char\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        \n",
    "        return [self.ix_to_char[ix] for ix in ixes]\n",
    "\n",
    "    def get_final_layers(self, word):\n",
    "        \"\"\" \n",
    "        Returnst the probability distribution on the final layer after each step of the RNN \n",
    "        \"\"\"\n",
    "        word = [self.training_chars[-1]] + word\n",
    "        inputs = [self.char_to_ix[ch] for ch in word]\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        pdists = []\n",
    "        hs[-1] = np.copy(self.last_h)\n",
    "        loss = 0\n",
    "        # forward pass\n",
    "        for t in xrange(len(inputs)):            \n",
    "            xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh) # hidden state\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            pdist = {}\n",
    "            for ch in self.char_to_ix:\n",
    "                ix = self.char_to_ix[ch]\n",
    "                pdist[ch] = ps[t][ix]\n",
    "                \n",
    "            pdists.append(pdist)\n",
    "        \n",
    "        return pdists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Here we train an RNN on the finite-state gramer used by Reber (1976) and later by Sevan-Schreiber, Cleeremans, and McClelland (1991) to explore the architecture of Graded State Machines of which RNNs are an example.\n",
    "\n",
    "Below is an image of the FSG taken from their paper. One minor change we make here is to run the FSG infinitely by returning to the beginning state after reaching the ending state.\n",
    "<img src=\"images/reberFSG.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8000, loss: 13.317639\n"
     ]
    }
   ],
   "source": [
    "reber76 = {\n",
    "    0: {(1, \"B\"): 1},\n",
    "    1: {(2, \"T\"): 0.5, (3, \"P\"): 0.5},\n",
    "    2: {(2, \"S\"): 0.5, (4, \"X\"): 0.5},\n",
    "    3: {(3, \"T\"): 0.5, (5, \"V\"): 0.5},\n",
    "    4: {(3, \"X\"): 0.5, (-1, \"S\"): 0.5},\n",
    "    5: {(4, \"P\"): 0.5, (-1, \"V\"): 0.5},\n",
    "    -1: {(0, \"E\"): 1},\n",
    "}\n",
    "\n",
    "#fsg = FSG(reber76, 0)\n",
    "#training_data = fsg.generate(200000, reset_curr = True)\n",
    "rnn = RNN(training_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the RNN sample grammatical words?\n",
    "\n",
    "We sample words from the RNN by using the square of the  probability distribution on the final layer to sample the next character in the word. We take the square of the probabilities to temper the flattening created by the softmax. When taking a sample the hidden layer is initialized to the last hidden layer during training and the input is initialized to the last character of the training data.\n",
    "\n",
    "A word is  considered grammatical if it could be generated by the FSG starting from the state where the training data ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 samples of length 100, with 1000 unique\n",
      "99.60 percent were grammatical\n"
     ]
    }
   ],
   "source": [
    "num = 1000\n",
    "sample_size = 100\n",
    "samples = [rnn.sample(sample_size) for _ in range(num)]\n",
    "grammatical = [s for s in samples if fsg.is_grammatical(s)]\n",
    "print \"%d samples of length %d, with %d unique\" %(num, sample_size, len(set([str(s) for s in samples])))\n",
    "print \"%.2f percent were grammatical\" %(len(grammatical)/float(num)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Does the RNN sample from the true probability distribution?\n",
    "\n",
    "We want to check that the RNN is not just sampling a proper subset of the words that can be generated by the grammar, but will sample each word in the grammar with the same proability as it occurs in the grammar.\n",
    "\n",
    "Let $P$ be the probability distribution over the next characters in the FSG and let $Q$ be the probability distribution on the final layer of the RNN. We compute the Kullback-Leibler divergence between the distributions. \n",
    "\n",
    "$$D_{KL}(P || Q) = \\sum_x P(x)\\log\\frac{Q(x)}{P(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average KL divergence over 100 steps is 0.0106\n"
     ]
    }
   ],
   "source": [
    "kl = lambda p, q: - sum([p[x]* np.log(q[x]/p[x]) for x in p if p[x] > 0])\n",
    "steps = 100\n",
    "word, pdists = fsg.generate_with_probdists(steps-1, reset_curr = False)\n",
    "qdists = rnn.get_final_layers(word)\n",
    "divs = []\n",
    "\n",
    "for i in range(len(pdists)):\n",
    "    p = pdists[i]\n",
    "    q = qdists[i]\n",
    "    div = kl(p,q)\n",
    "    divs.append(div)\n",
    "\n",
    "print \"The average KL divergence over %d steps is %.4f\" %(steps, np.average(divs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, consider the average KL divergence between $P$ and $(1-a)P + a U$, a mixture of $P$ and the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a \t Average KL Divergence\n",
      "--------------------------------------------------\n",
      "0.0 \t 0.0000\n",
      "0.1 \t 0.0896\n",
      "0.2 \t 0.1881\n",
      "0.3 \t 0.2973\n",
      "0.4 \t 0.4199\n",
      "0.5 \t 0.5596\n",
      "0.6 \t 0.7221\n",
      "0.7 \t 0.9163\n",
      "0.8 \t 1.1575\n",
      "0.9 \t 1.4759\n",
      "1.0 \t 1.9459\n"
     ]
    }
   ],
   "source": [
    "flat = {ch: 1.0/rnn.vocab_size for ch in rnn.char_to_ix.keys()}\n",
    "inc = .1\n",
    "a = 0\n",
    "print \"a \\t Average KL Divergence\"\n",
    "print \"-\"*50\n",
    "while a <= 1:\n",
    "    for i in range(len(pdists)):\n",
    "        p = pdists[i]\n",
    "        noisies = []\n",
    "        q = {x: (1-a) * p[x] + a * flat[x] for x in flat.keys()}     \n",
    "        noisies.append(kl(p, q))\n",
    "    \n",
    "    print \"%.1f \\t %.4f\" %(a, np.average(noisies))\n",
    "    a += inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
